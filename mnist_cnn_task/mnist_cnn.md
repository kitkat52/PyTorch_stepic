Task 4.4 - Классификация рукописных чисел сверточной нейросетью

Дана сеть архитектуры LeNet с двумя сверточными слоями, где ядро свертки размера 5x5. Также используется AvgPooling и функция активации гиперболический тангенс. В конце три полносвязных слоя.

[//]: # (Image References)
[image1]: ./img/loss_adam.png "loss adam"

[//]: # (Image References)
[image2]: ./img/acc_adam.png "accuracy adam"

[//]: # (Image References)
[image3]: ./img/acc_sgd.png "accuracy SGD"

[//]: # (Image References)
[image4]: ./img/loss_sgd.png "loss SGD"

[//]: # (Image References)
[image5]: ./img/acc_sgd_0.8_0.01.png "accuracy SGDM"

[//]: # (Image References)
[image6]: ./img/loss_sgd_0.8_0.01.png "loss SGDM"

[//]: # (Image References)
[image7]: ./img/loss_avg_elu.png "loss overfit"

[//]: # (Image References)
[image8]: ./img/time.png "time"



1. Попробуйте добиться качества 0.992  на данном датасете (в максимуме на валидации)

Сравниваются алгоритмы Adam, стохастический градиентный спуск и СГД с моментом (значения момента [0.7, 0.8, 0.9] и скорости обучения [0.01, 0.001]).

- Adam, lr=0.001

![alt text][image1]

![alt text][image2]

- Стохастический градиентный спуск, lr=0.001

![alt text][image3]

![alt text][image4]

- СГД с моментом

![alt text][image5]

![alt text][image6]

Выбран алгоритм оптимизации SGD с моментом 0.8 и скоростью обучения 0.01 на 200 эпох.

2. Появляется ли переобучение при увеличении количества эпох?
Сеть с тем же количеством слоев, но в качестве функции активации используется ELU.

![alt text][image7]

На графике видно, что после приблизительно 15-20 эпохи идет переобучение.
Более подробно в [файле](./overfit.ipynb)

3. Как добавление различных слоев влияет на скорость обучения (какие слои быстрее: сверточные или полносвязные)?
Исходная архитектура сети соответствует сети из предыдущего пункта.
Далее поочередно дополняется полносвязным слоем и сверточным слоем, каждый раз замеряется время работы.

Построен график изменения времени вычисления сети.

![alt text][image8]

Можно сказать, что вычисление полносвязного слоя быстрее вычисления сверточного.

4. Результат обучения может меняться в от запуска к запуску. Проанализируйте дисперсию целевой метрики от запуска к запуску. Сколько запусков достаточно произвести, перед тем как утверждать, что одна архитектура лучше другой?

Было проанализировано три модели сети:
- [mod1](./mod1_plot.ipynb): исходная архитектура сети с функцией активации ELU, MaxPooling и Adam в качестве алгоритма оптимизации
- [mod2](./mod2_plot.ipynb): исходная архитектура сети с функцией активации ELU, AvgPooling и SGDM в качестве алгоритма оптимизации
- [mod3](./mod3_plot.ipynb): модель mod2 с добавлением одного сверточного слоя (ядро 3x3) и полносвязного (400->*200*->120->84->10)

Каждая модель запущена 10 раз с разным значением сида в рандоме.
Построен график изменения дисперсии по эпохам. Исходя из построенных графиков, достаточно 6-7 раз чтобы выделить ту или иную архитектуру сети в качетстве наилучшей. Из представленных трех моделей последняя стабильно показывала результат по метрике на валидации выше 0.992.

